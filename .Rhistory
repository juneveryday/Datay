prediction_label <-
data.frame(max.col(PD.neural.predict)) %>%
mutate(pred=labels[max.col(PD.neural.predict)]) %>%
select(2) %>%
unlist()
PD.neural.conf_matrix <- table(observed = PD.neural.test$Class, predicted = prediction_label)
PD.neural.conf_matrix
# accuracy 계산
PD.neural.accuracy <- sum(diag(PD.neural.conf_matrix)) / sum(PD.neural.conf_matrix)
PD.neural.accuracy
# "Classifier" 열에 "ANN"을 추가
classifier_table_copy$Classifier <- rep("ANN", nrow(classifier_table_copy))
# PD.neural의 정확도를 "Accuracy" 열에 추가
classifier_table_copy$Accuracy <- PD.neural.accuracy
classifier_table_copy
classifier_table
classifier_table_copy = classifier_table
# "Classifier" 열에 "ANN"을 추가
classifier_table_copy$Classifier <- rep("ANN", nrow(classifier_table_copy))
classifier_table_copy
classifier_table
classifier_table_copy = classifier_table
classifiers_name <- c("Decision Tree", "Naive Bayes", "Bagging", "Boosting", "Random Forest", "ANN")
accuracy_of_classifiers <- c(PD.tree.accuracy,
PD.nav.accuracy,
PD.bag.accuracy,
PD.boost.accuracy,
PD.randomforest.accuracy,
PD.neural.accuracy)
accuracy_of_classifiers_rounded <- round(accuracy_of_classifiers, 4)
classifier_table2 <- data.frame(Classifier = classifiers_name, Accuracy = accuracy_of_classifiers_rounded)
classifier_table2
# Question 12
PD.linear <- lm(Class ~ ., data = PD.new.train)
PD.linear.prediction_label <-
data.frame(max.col(PD.linear)) %>%
mutate(pred=labels[max.col(PD.linear)]) %>%
select(2) %>%
unlist()
# Question 12
PD.linear <- lm(Class ~ ., data = PD.new.train)
predicted_values <- predict(PD.linear, newdata = PD.new.train, type = "response")
binary_predictions <- ifelse(predicted_values > 0.5, 1, 0)
confusion_matrix <- table(PD.train$Class, binary_predictions)
accuracy <- sum(diag(confusion_matrix)) / sum(confusion_matrix)
accuracy
summary(PD.linear)
PD.svm <- svm(Class ~ ., PD.train, kernel = "linear")
PD.svm
PD.sevm.predict = predict(PD.svm, PD.test)
PD.svm.predict = predict(PD.svm, PD.test)
table(actual = PD.test$Class, predicted = PD.svm.predict)
PD.svm.accuracy <- sum(diag(PD.svm.conf_matrix)) / sum(PD.svm.conf_matrix)
PD.svm.conf_matrix <- table(actual = PD.test$Class, predicted = PD.svm.predict)
PD.svm.accuracy <- sum(diag(PD.svm.conf_matrix)) / sum(PD.svm.conf_matrix)
PD.svm.accuracy
PD.svm.conf_matrix <- table(actual = PD.test$Class, predicted = PD.svm.predict)
PD.svm.conf_matrix
classifiers_name <- c("Decision Tree", "Naive Bayes", "Bagging", "Boosting", "Random Forest", "ANN", "SVM")
accuracy_of_classifiers <- c(PD.tree.accuracy,
PD.nav.accuracy,
PD.bag.accuracy,
PD.boost.accuracy,
PD.randomforest.accuracy,
PD.neural.accuracy,
PD.svm.accuracy)
accuracy_of_classifiers_rounded <- round(accuracy_of_classifiers, 4)
classifier_table2 <- data.frame(Classifier = classifiers_name, Accuracy = accuracy_of_classifiers_rounded)
classifier_table2
AUC_table
library(ROCR)
PD.pred.svm <- predict(PD.svm, PD.test, type="prob")
PDSVM.pred <- prediction (PD.pred.svm[,2],PD.test$Class)
PD.pred.svm <- predict(PD.svm, PD.test, type="prob")
D.pred.svm
PD.pred.svm
PDSVM.pred <- ROCR::prediction (PD.pred.svm[,2],PD.test$Class)
PD.svm <- svm(Class ~ ., PD.train, kernel = "linear", probability = TRUE)
PD.svm.predict = predict(PD.svm, PD.test)
PD.svm.conf_matrix <- table(actual = PD.test$Class, predicted = PD.svm.predict)
PD.svm.accuracy <- sum(diag(PD.svm.conf_matrix)) / sum(PD.svm.conf_matrix)
classifiers_name <- c("Decision Tree", "Naive Bayes", "Bagging", "Boosting", "Random Forest", "ANN", "SVM")
accuracy_of_classifiers <- c(PD.tree.accuracy,
PD.nav.accuracy,
PD.bag.accuracy,
PD.boost.accuracy,
PD.randomforest.accuracy,
PD.neural.accuracy,
PD.svm.accuracy)
accuracy_of_classifiers_rounded <- round(accuracy_of_classifiers, 4)
classifier_table2 <- data.frame(Classifier = classifiers_name, Accuracy = accuracy_of_classifiers_rounded)
classifier_table2
PD.svm <- svm(Class ~ ., PD.train, kernel = "linear", probability = TRUE)
PD.svm.predict = predict(PD.svm, PD.test)
PD.svm.conf_matrix <- table(actual = PD.test$Class, predicted = PD.svm.predict)
PD.svm.accuracy <- sum(diag(PD.svm.conf_matrix)) / sum(PD.svm.conf_matrix)
classifiers_name <- c("Decision Tree", "Naive Bayes", "Bagging", "Boosting", "Random Forest", "ANN", "SVM")
accuracy_of_classifiers <- c(PD.tree.accuracy,
PD.nav.accuracy,
PD.bag.accuracy,
PD.boost.accuracy,
PD.randomforest.accuracy,
PD.neural.accuracy,
PD.svm.accuracy)
accuracy_of_classifiers_rounded <- round(accuracy_of_classifiers, 4)
classifier_table2 <- data.frame(Classifier = classifiers_name, Accuracy = accuracy_of_classifiers_rounded)
classifier_table2
PD.svm <- svm(Class ~ ., PD.train, kernel = "linear")
PD.svm.predict = predict(PD.svm, PD.test)
PD.svm.conf_matrix <- table(actual = PD.test$Class, predicted = PD.svm.predict)
PD.svm.accuracy <- sum(diag(PD.svm.conf_matrix)) / sum(PD.svm.conf_matrix)
classifiers_name <- c("Decision Tree", "Naive Bayes", "Bagging", "Boosting", "Random Forest", "ANN", "SVM")
accuracy_of_classifiers <- c(PD.tree.accuracy,
PD.nav.accuracy,
PD.bag.accuracy,
PD.boost.accuracy,
PD.randomforest.accuracy,
PD.neural.accuracy,
PD.svm.accuracy)
accuracy_of_classifiers_rounded <- round(accuracy_of_classifiers, 4)
classifier_table2 <- data.frame(Classifier = classifiers_name, Accuracy = accuracy_of_classifiers_rounded)
classifier_table2
PD.svm.pred <- predict(PD.svm, PD.test, probability = TRUE)
PD.svm.pred <- predict(PD.svm, PD.test)
PD.svm.prob <- attr(PD.svm.predict, "probabilities")
PD.svm.prob
PDSVM.pred <- ROCR::prediction (PD.pred.svm[,2],PD.test$Class)
PDSVM.pred <- ROCR::prediction (PD.svm.prob[,2],PD.test$Class)
PD.svm.predict = predict(PD.svm, PD.test)
PD.svm.prob <- attr(PD.svm.predict, "probabilities")
PDSVM.pred <- ROCR::prediction (PD.svm.prob[,2],PD.test$Class)
PDSVM.perf <- performance(PDSVM.pred, "tpr","fpr")
PD.svm <- svm(Class ~ ., PD.train, kernel = "linear")
PD.svm.predict = predict(PD.svm, PD.test)
PD.svm.conf_matrix <- table(actual = PD.test$Class, predicted = PD.svm.predict)
PD.svm.accuracy <- sum(diag(PD.svm.conf_matrix)) / sum(PD.svm.conf_matrix)
classifiers_name <- c("Decision Tree", "Naive Bayes", "Bagging", "Boosting", "Random Forest", "ANN", "SVM")
accuracy_of_classifiers <- c(PD.tree.accuracy,
PD.nav.accuracy,
PD.bag.accuracy,
PD.boost.accuracy,
PD.randomforest.accuracy,
PD.neural.accuracy,
PD.svm.accuracy)
accuracy_of_classifiers_rounded <- round(accuracy_of_classifiers, 4)
classifier_table2 <- data.frame(Classifier = classifiers_name, Accuracy = accuracy_of_classifiers_rounded)
classifier_table2
library(ROCR)
PD.svm.prob <- attr(PD.svm.predict, "probabilities")
PDSVM.pred <- ROCR::prediction (PD.svm.prob[,2],PD.test$Class)
str(PD.svm.prob)
# 필요한 패키지 로드
library(e1071)
library(ROCR)
# SVM 모델 훈련 (확률 예측을 위해 probability = TRUE 사용)
PD.svm <- svm(Class ~ ., data = PD.train, kernel = "linear", probability = TRUE)
# 예측 수행 (확률 포함)
PD.svm.predict <- predict(PD.svm, PD.test, probability = TRUE)
# 예측된 확률 추출
PD.svm.prob <- attr(PD.svm.predict, "probabilities")
# 클래스 1에 대한 확률 추출 (이진 분류의 경우)
# 첫 번째 열은 클래스 1에 대한 확률
predictions <- PD.svm.prob[,1]
# ROCR 패키지를 사용하여 ROC 곡선 그리기
PDSVM.pred <- ROCR::prediction(predictions, PD.test$Class)
PDSVM.perf <- ROCR::performance(PDSVM.pred, "tpr", "fpr")
# ROC 곡선 시각화
plot(PDSVM.perf, col = "blue", main = "ROC Curve for SVM")
abline(a = 0, b = 1, lty = 2, col = "red")
# 클래스 1에 대한 확률 추출 (이진 분류의 경우)
# 첫 번째 열은 클래스 1에 대한 확률
predictions <- PD.svm.prob[,2]
# ROCR 패키지를 사용하여 ROC 곡선 그리기
PDSVM.pred <- ROCR::prediction(predictions, PD.test$Class)
PDSVM.perf <- ROCR::performance(PDSVM.pred, "tpr", "fpr")
# ROC 곡선 시각화
plot(PDSVM.perf, col = "blue", main = "ROC Curve for SVM")
abline(a = 0, b = 1, lty = 2, col = "red")
# 필요한 패키지 로드
library(e1071)
library(ROCR)
# SVM 모델 훈련 (확률 예측을 위해 probability = TRUE 사용)
PD.svm <- svm(Class ~ ., data = PD.train, kernel = "linear", probability = TRUE)
# 예측 수행 (확률 포함)
PD.svm.predict <- predict(PD.svm, PD.test, probability = TRUE)
# 예측된 확률 추출
PD.svm.prob <- attr(PD.svm.predict, "probabilities")
# 클래스 1에 대한 확률 추출 (이진 분류의 경우)
# 첫 번째 열은 클래스 1에 대한 확률
predictions <- PD.svm.prob[,2]
# ROCR 패키지를 사용하여 ROC 곡선 그리기
PDSVM.pred <- ROCR::prediction(predictions, PD.test$Class)
PDSVM.perf <- ROCR::performance(PDSVM.pred, "tpr", "fpr")
# ROC 곡선 시각화
plot(PDSVM.perf, col = "blue", main = "ROC Curve for SVM")
abline(a = 0, b = 1, lty = 2, col = "red")
#AUC - SVM
PDSVM.auc_value <- performance(PDSVM.pred, "auc")
PDSVM.auc <- PDSVM.auc_value@y.values[[1]]
PDSVM.auc
knitr::opts_chunk$set(echo = TRUE)
library(dplyr)
library(pROC)
PD.neural.predict = predict(trial, PD.neural.test)
labels <- c('0','1')
prediction_checker <- labels[max.col(PD.neural.predict)]
PD.neural.conf_matrix <- table(observed = PD.neural.test$Class, predicted = prediction_checker)
PD.neural.conf_matrix
# accuracy calculate
PD.neural.accuracy <- sum(diag(PD.neural.conf_matrix)) / sum(PD.neural.conf_matrix)
cat("ANN accuracy : ", PD.neural.accuracy)
classifiers_name <- c("Decision Tree", "Naive Bayes", "Bagging", "Boosting", "Random Forest", "ANN")
accuracy_of_classifiers <- c(PD.tree.accuracy,
PD.nav.accuracy,
PD.bag.accuracy,
PD.boost.accuracy,
PD.randomforest.accuracy,
PD.neural.accuracy)
accuracy_of_classifiers_rounded <- round(accuracy_of_classifiers, 4)
classifier_table2 <- data.frame(Classifier = classifiers_name, Accuracy = accuracy_of_classifiers_rounded)
classifier_table2
plot(trial)
View(PD.bag)
dim(data_dtm)
debugSource("~/Documents/Monash University/FIT 3152 Data Analysis/Data-Analysis-Project-3/Brainstorming.R", echo=TRUE)
# reset
rm(list = ls())
# bringing library
library(slam)
library(tm)
library(SnowballC)
library(proxy)
library(igraph)
# text file path checking
data_cname = file.path(".","Data_text")
# Question 2
# make corpus
set.seed("31994695")
data_docs = Corpus(DirSource(data_cname))
data_docs
# Question 3
# Start Making Token
# summary(food_docs)
juneremoveChars <- content_transformer(function(x, pattern) gsub(pattern, "", x))
## Tokenisation
data_docs <- tm_map(data_docs, removeNumbers)
data_docs <- tm_map(data_docs, removePunctuation)
data_docs <- tm_map(data_docs, content_transformer(tolower))
data_docs <- tm_map(data_docs, juneremoveChars, "’s")
data_docs <- tm_map(data_docs, juneremoveChars, "’ve")
data_docs <- tm_map(data_docs, juneremoveChars, "’d")
data_docs <- tm_map(data_docs, juneremoveChars, "’m")
data_docs <- tm_map(data_docs, juneremoveChars, "n't")
data_docs <- tm_map(data_docs, juneremoveChars, "like")
data_docs <- tm_map(data_docs, juneremoveChars, "can")
data_docs <- tm_map(data_docs, juneremoveChars, "take")
data_docs <- tm_map(data_docs, juneremoveChars, "also")
setwd("~/Documents/Monash University/FIT 3152 Data Analysis/Data-Analysis-Project-3")
# reset
rm(list = ls())
# bringing library
library(slam)
library(tm)
library(SnowballC)
library(proxy)
library(igraph)
# text file path checking
data_cname = file.path(".","Data_text")
# Question 2
# make corpus
set.seed("31994695")
data_docs = Corpus(DirSource(data_cname))
data_docs
# Question 3
# Start Making Token
# summary(food_docs)
juneremoveChars <- content_transformer(function(x, pattern) gsub(pattern, "", x))
## Tokenisation
data_docs <- tm_map(data_docs, removeNumbers)
data_docs <- tm_map(data_docs, removePunctuation)
data_docs <- tm_map(data_docs, content_transformer(tolower))
data_docs <- tm_map(data_docs, juneremoveChars, "’s")
data_docs <- tm_map(data_docs, juneremoveChars, "’ve")
data_docs <- tm_map(data_docs, juneremoveChars, "’d")
data_docs <- tm_map(data_docs, juneremoveChars, "’m")
data_docs <- tm_map(data_docs, juneremoveChars, "n't")
data_docs <- tm_map(data_docs, juneremoveChars, "like")
data_docs <- tm_map(data_docs, juneremoveChars, "can")
data_docs <- tm_map(data_docs, juneremoveChars, "take")
data_docs <- tm_map(data_docs, juneremoveChars, "also")
data_docs <- tm_map(data_docs, juneremoveChars, "one")
data_docs <- tm_map(data_docs, juneremoveChars, "year")
data_docs <- tm_map(data_docs, juneremoveChars, "–")
data_docs <- tm_map(data_docs, juneremoveChars, "“")
data_docs <- tm_map(data_docs, juneremoveChars, "”")
# Filter words
# Remove stop words and white space
data_docs <- tm_map(data_docs, removeWords, stopwords("english"))
data_docs <- tm_map(data_docs, stripWhitespace)
# Stem
data_docs<- tm_map(data_docs, stemDocument, language = "english")
# Create document term matrix
set.seed("31994695")
data_dtm <- DocumentTermMatrix(data_docs)
dim(data_dtm)
# Remove sparse terms
# Remove columns with 60% empty (0) cells
data_dtm <- removeSparseTerms(data_dtm, sparse = 0.5)
dim(data_dtm)
# Cosine distance between each document for clustering.
set.seed("31994695")
dim(data_dtm)
# Cosine distance between each document for clustering.
set.seed("31994695")
dtms = as.matrix(data_dtm)
distmatrix = proxy::dist(dtms, method = "cosine")
fit = hclust(distmatrix, method = "ward.D")
plot(fit, hang = -1, main = "Question 4, Clustering Dendrogram")
fit <- hclust(distmatrix, method = "ward.D")
fit
# using cluster object "fit" create required number of clusters.
cutfit <- cutree(fit, k = 3)
# Create vector of topic labels in same order as corpus
topics = c("food", "food", "food","food","food",
"tech", "tech", "tech", "tech", "tech",
"world", "world", "world", "world", "world")
groups = cutree(fit, k = 3)
cluster_table <- table(GroupNames = topics, Clusters = groups)
TA =  as.data.frame.matrix(table(GroupNames = topics, Clusters = groups))
TA = TA[,c(2,1,3)]
TA
TA_matrix <- as.matrix(TA)
diag_TA <- diag(TA_matrix)
accuracy <- sum(diag_TA) / sum(TA_matrix)
accuracy
# convert to binary matrix
dtmsx = as.matrix((dtms > 0) + 0)
# multiply binary matrix by its transpose
ByAbsMatrix = dtmsx %*% t(dtmsx)
# make leading diagonal zero
diag(ByAbsMatrix) = 0
# Create graph object
set.seed("31994695")
Q5_SM_network = graph_from_adjacency_matrix(ByAbsMatrix, mode = "undirected", weighted = TRUE)
# Plot the Basic model
set.seed("31994695")
plot(Q5_SM_network,
main = "Q5 Single Basic Mode Network")
# Get the weights
Q5_SM_network_weight = E(Q5_SM_network)$weight
# Create color palette function
color_picker = colorRampPalette(c("pink","lightblue","green"))
# Generate edge colors based on weights
Q5_edge_colors <- color_picker(length(Q5_SM_network_weight))[as.numeric(cut(Q5_SM_network_weight, breaks = length(Q5_SM_network_weight)))]
set.seed("31994695")
# Plot the graph
plot(Q5_SM_network,
edge.label = Q5_SM_network_weight,
edge.color = Q5_edge_colors,
edge.width = 1,
main = "Q5 Single Final Mode Network")
# Question 6
set.seed("31994695")
ByTokenMatrix = t(dtmsx) %*% dtmsx
# make leading diagonal zero
diag(ByTokenMatrix) = 0
# Create graph object
set.seed("31994695")
Q6_TK_network = graph_from_adjacency_matrix(ByTokenMatrix, mode = "undirected", weighted = TRUE)
# plot the basic model
set.seed("31994695")
plot(Q6_TK_network,
main = "Q6 Single Basic Mode Network")
# Get the weights
Q6_TK_network_weight = E(Q6_TK_network)$weight
# Generate edge colors based on weights
Q6_edge_colors <- color_picker(length(Q6_TK_network_weight))[as.numeric(cut(Q6_TK_network_weight, breaks = length(Q6_TK_network_weight)))]
set.seed("31994695")
# Plot the graph
plot(Q6_TK_network,
edge.label = Q6_TK_network_weight,
edge.color = Q6_edge_colors,
edge.width = 1,
main = "Q6 Token Final Mode Network")
# Question 7
# start with documnet term matrix dtms
dtmsa = as.data.frame(dtms) # clone dtms
# reset
rm(list = ls())
# bringing library
library(slam)
library(tm)
library(SnowballC)
library(proxy)
library(igraph)
# text file path checking
data_cname = file.path(".","Data_text")
# Question 2
# make corpus
set.seed("31994695")
data_docs = Corpus(DirSource(data_cname))
data_docs
# Question 3
# Start Making Token
# summary(food_docs)
juneremoveChars <- content_transformer(function(x, pattern) gsub(pattern, "", x))
## Tokenisation
data_docs <- tm_map(data_docs, removeNumbers)
data_docs <- tm_map(data_docs, removePunctuation)
data_docs <- tm_map(data_docs, content_transformer(tolower))
data_docs <- tm_map(data_docs, juneremoveChars, "’s")
data_docs <- tm_map(data_docs, juneremoveChars, "’ve")
data_docs <- tm_map(data_docs, juneremoveChars, "’d")
data_docs <- tm_map(data_docs, juneremoveChars, "’m")
data_docs <- tm_map(data_docs, juneremoveChars, "n't")
data_docs <- tm_map(data_docs, juneremoveChars, "like")
data_docs <- tm_map(data_docs, juneremoveChars, "can")
data_docs <- tm_map(data_docs, juneremoveChars, "take")
data_docs <- tm_map(data_docs, juneremoveChars, "also")
data_docs <- tm_map(data_docs, juneremoveChars, "one")
data_docs <- tm_map(data_docs, juneremoveChars, "year")
data_docs <- tm_map(data_docs, juneremoveChars, "–")
data_docs <- tm_map(data_docs, juneremoveChars, "“")
data_docs <- tm_map(data_docs, juneremoveChars, "”")
# Filter words
# Remove stop words and white space
data_docs <- tm_map(data_docs, removeWords, stopwords("english"))
data_docs <- tm_map(data_docs, stripWhitespace)
# Stem
data_docs<- tm_map(data_docs, stemDocument, language = "english")
# Create document term matrix
set.seed("31994695")
data_dtm <- DocumentTermMatrix(data_docs)
dim(data_dtm)
# Remove sparse terms
# Remove columns with 60% empty (0) cells
data_dtm <- removeSparseTerms(data_dtm, sparse = 0.5)
dim(data_dtm)
# Cosine distance between each document for clustering.
set.seed("31994695")
dtms = as.matrix(data_dtm)
distmatrix = proxy::dist(dtms, method = "cosine")
fit = hclust(distmatrix, method = "ward.D")
plot(fit, hang = -1, main = "Question 4, Clustering Dendrogram")
fit <- hclust(distmatrix, method = "ward.D")
fit
# using cluster object "fit" create required number of clusters.
cutfit <- cutree(fit, k = 3)
# Create vector of topic labels in same order as corpus
topics = c("food", "food", "food","food","food",
"tech", "tech", "tech", "tech", "tech",
"world", "world", "world", "world", "world")
groups = cutree(fit, k = 3)
cluster_table <- table(GroupNames = topics, Clusters = groups)
TA =  as.data.frame.matrix(table(GroupNames = topics, Clusters = groups))
TA = TA[,c(2,1,3)]
TA
TA_matrix <- as.matrix(TA)
diag_TA <- diag(TA_matrix)
accuracy <- sum(diag_TA) / sum(TA_matrix)
accuracy
# convert to binary matrix
dtmsx = as.matrix((dtms > 0) + 0)
# multiply binary matrix by its transpose
ByAbsMatrix = dtmsx %*% t(dtmsx)
# reset
rm(list = ls())
# bringing library
library(slam)
library(tm)
library(SnowballC)
library(proxy)
library(igraph)
# text file path checking
data_cname = file.path(".","Data_text")
# Question 2
# make corpus
set.seed("31994695")
data_docs = Corpus(DirSource(data_cname))
data_docs
# Question 3
# Start Making Token
# summary(food_docs)
juneremoveChars <- content_transformer(function(x, pattern) gsub(pattern, "", x))
## Tokenisation
data_docs <- tm_map(data_docs, removeNumbers)
data_docs <- tm_map(data_docs, removePunctuation)
data_docs <- tm_map(data_docs, content_transformer(tolower))
data_docs <- tm_map(data_docs, juneremoveChars, "’s")
data_docs <- tm_map(data_docs, juneremoveChars, "’ve")
data_docs <- tm_map(data_docs, juneremoveChars, "’d")
data_docs <- tm_map(data_docs, juneremoveChars, "’m")
data_docs <- tm_map(data_docs, juneremoveChars, "n't")
data_docs <- tm_map(data_docs, juneremoveChars, "like")
data_docs <- tm_map(data_docs, juneremoveChars, "can")
data_docs <- tm_map(data_docs, juneremoveChars, "take")
data_docs <- tm_map(data_docs, juneremoveChars, "also")
data_docs <- tm_map(data_docs, juneremoveChars, "one")
data_docs <- tm_map(data_docs, juneremoveChars, "year")
data_docs <- tm_map(data_docs, juneremoveChars, "–")
data_docs <- tm_map(data_docs, juneremoveChars, "“")
data_docs <- tm_map(data_docs, juneremoveChars, "”")
# Filter words
# Remove stop words and white space
data_docs <- tm_map(data_docs, removeWords, stopwords("english"))
data_docs <- tm_map(data_docs, stripWhitespace)
# Stem
data_docs<- tm_map(data_docs, stemDocument, language = "english")
# Create document term matrix
set.seed("31994695")
data_dtm <- DocumentTermMatrix(data_docs)
dim(data_dtm)
